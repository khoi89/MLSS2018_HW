{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Assignment: Convolutional Neural Network (CNN)\n",
    "\n",
    "**[Duke Community Standard](http://integrity.duke.edu/standard.html): By typing your name below, you are certifying that you have adhered to the Duke Community Standard in completing this assignment.**\n",
    "\n",
    "Name: Khoi D. Vo </br>\n",
    "Date: June 25, 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network\n",
    "\n",
    "Build a 2-layer CNN for MNIST digit classfication. Feel free to play around with the model architecture and see how the training time/performance changes, but to begin, try the following:\n",
    "\n",
    "Image -> Convolution (32 5x5 filters) -> nonlinearity (ReLU) ->  (2x2 max pool) -> Convolution (64 5x5 filters) -> nonlinearity (ReLU) -> (2x2 max pool) -> fully connected (256 hidden units) -> nonlinearity (ReLU) -> fully connected (10 hidden units) -> softmax\n",
    "\n",
    "Some tips:\n",
    "- The CNN model might take a while to train. Depending on your machine, you might expect this to take up to half an hour. If you see your validation performance start to plateau, you can kill the training.\n",
    "\n",
    "- Since CNNs a more complex than the logistic regression and MLP models you've worked with before, so you may find it helpful to use a more advanced optimizer. You're model will train faster if you use [`tf.train.AdamOptimizer`](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer) instead of `tf.train.GradientDescentOptimizer`. A learning rate of 1e-4 is a good starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing necessary toolboxes & image sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting official/mnist/dataset.py/train-images-idx3-ubyte.gz\n",
      "Extracting official/mnist/dataset.py/train-labels-idx1-ubyte.gz\n",
      "Extracting official/mnist/dataset.py/t10k-images-idx3-ubyte.gz\n",
      "Extracting official/mnist/dataset.py/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "from tqdm import trange         \n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"official/mnist/dataset.py\", one_hot=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constructing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def conv(x, w, b, s):\n",
    "    x = tf.nn.conv2d(x, w, strides=[1,s,s,1], padding='SAME')\n",
    "    x = tf.nn.relu(tf.add(x, b))\n",
    "    return x\n",
    "\n",
    "def conv_network(x, w, b, s):\n",
    "    x = tf.reshape(x, [-1, 28, 28, 1])\n",
    "    \n",
    "    # convolution layer 1\n",
    "    conv1 = conv(x, w['wc_1'], b['bc_1'], s) \n",
    "    conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') \n",
    "    \n",
    "    # convolution layer 2\n",
    "    conv2 = conv(conv1, w['wc_2'], b['bc_2'], s) # [-1,14,14,64]\n",
    "    conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') \n",
    "\n",
    "    # fully connected layers\n",
    "    fc_1 = tf.reshape(conv2, [-1, w['wfc_1'].get_shape().as_list()[0]])\n",
    "    fc_1 = tf.nn.relu(tf.matmul(fc_1, w['wfc_1']) + b['bfc_1'])\n",
    "    fc_out = tf.matmul(fc_1, w['wfc_out']) + b['bfc_out']\n",
    "    \n",
    "    return fc_out\n",
    "\n",
    "# Defining weights and biases\n",
    "weights = {'wc_1': tf.Variable(tf.random_normal([5, 5, 1, 32])), # convolution filter 1\n",
    "           'wc_2': tf.Variable(tf.random_normal([5, 5, 32, 64])), # convolution filter 2\n",
    "           'wfc_1': tf.Variable(tf.random_normal([7*7*64, 256])), # fully connected, 7*7*64 inputs (two poolings of 2x2, original image 28x28 --> 7x7x64), 256 outputs\n",
    "           'wfc_out': tf.Variable(tf.random_normal([256, 10]))}\n",
    "\n",
    "biases = {'bc_1': tf.Variable(tf.random_normal([32])),\n",
    "          'bc_2': tf.Variable(tf.random_normal([64])),\n",
    "          'bfc_1': tf.Variable(tf.random_normal([256])),\n",
    "          'bfc_out': tf.Variable(tf.random_normal([10]))}\n",
    "\n",
    "stride = 1\n",
    "\n",
    "# Define input placeholder & logit\n",
    "X = tf.placeholder(tf.float32, [None, 784]) # 784 pixels per image\n",
    "logits = conv_network(X, weights, biases, stride)\n",
    "\n",
    "# Define loss and optimizer functions\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "loss_fn = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "optimizer_fn = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a session object and initialize all graph variables\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [09:04<00:00,  1.09s/it]\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "batch_size = 1000\n",
    "for epoch in trange(500):\n",
    "    batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "    sess.run(optimizer_fn, feed_dict={X: batch_x, Y: batch_y})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9697999954223633\n"
     ]
    }
   ],
   "source": [
    "# Test trained model\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Test accuracy: {0}' \\\n",
    "      .format(sess.run(accuracy, feed_dict={X: mnist.test.images, Y: mnist.test.labels})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
